{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import bcolz\n",
    "import keras\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors, word2vec\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bcolz_array_iterator import BcolzArrayIterator\n",
    "from string import punctuation\n",
    "from keras.layers import InputSpec, Layer, Input, Dense, merge\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers import Bidirectional, GRU, LSTM\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Merge\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Reshape, Lambda, Conv2D, Conv1D\n",
    "from keras.layers import merge\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, BaseLogger, ProgbarLogger\n",
    "\n",
    "np.random.seed(7)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# !cd en_core_web_md-1.2.1 && python setup.py install\n",
    "# !python -m spacy link en_core_web_md en_core_web_md\n",
    "\n",
    "# !pip install gensim nltk keras pandas bcolz h5py spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30_64_0_1_0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "bcolz_chunklen = 64\n",
    "stem = 0\n",
    "init_random = 1\n",
    "stop_words = 0\n",
    "w2v_provider = 'glove'\n",
    "bcolz_prefix = '%s_%s_%s_%s_%s' % (MAX_SEQUENCE_LENGTH,\n",
    "                                   bcolz_chunklen,\n",
    "                                   stem,\n",
    "                                   init_random,\n",
    "                                   stop_words)\n",
    "bcolz_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404288, 6)\n",
      "(2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv.zip')\n",
    "df = df[(df.question1.isnull()  == False) & (df.question2.isnull() == False)]\n",
    "train_df = df.iloc[:360000]\n",
    "val_df = df.iloc[360000:]\n",
    "print(df.shape)\n",
    "df = None\n",
    "test_df = pd.read_csv('test.csv.zip')\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 3.5 s, total: 2min 52s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if w2v_provider == 'glove':\n",
    "    w2v = {}\n",
    "    w2v_path = 'glove.840B.300d.txt'\n",
    "    with open(w2v_path) as f_:\n",
    "        for line in f_:\n",
    "            try:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                w2v[word] = coefs\n",
    "            except Exception as e:\n",
    "                continue\n",
    "else:\n",
    "    w2v_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "    w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    if text is np.nan:\n",
    "        text = ''\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "#     text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"\\[math\\].*\\[\\/math\\]\", 'math', text)\n",
    "    text = re.sub(r\"\\[code\\].*\\[\\/code\\]\", 'code', text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\":\", \"\", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_vec(word, w2v, custom_w2v, init_random=False):\n",
    "    if word in w2v:\n",
    "        return w2v[word]\n",
    "    elif word in custom_w2v:\n",
    "        return custom_w2v[word]\n",
    "    else:\n",
    "        if init_random:\n",
    "            custom_w2v[word] = np.random.normal(scale=.5, size=[1,300])\n",
    "        else:\n",
    "            custom_w2v[word] = np.zeros([1,300])\n",
    "        return custom_w2v[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_w2v = {}\n",
    "def save_bcolz_array(data_iter, rootdir):\n",
    "    carr = bcolz.carray([next(data_iter)], chunklen=64,\n",
    "                        rootdir=rootdir, mode='w')\n",
    "    for chunk in data_iter:\n",
    "        carr.append(chunk)\n",
    "    carr.flush()\n",
    "    \n",
    "\n",
    "def emb_iter(data):\n",
    "    for qid, question in enumerate(data):\n",
    "        emb = np.zeros([MAX_SEQUENCE_LENGTH,300]) \n",
    "        for idx, word in enumerate(text_to_wordlist(question,\n",
    "                                                    remove_stopwords=stop_words,\n",
    "                                                    stem_words=stem)):\n",
    "            if idx < MAX_SEQUENCE_LENGTH:\n",
    "                emb[idx,:] = word_to_vec(word, w2v, custom_w2v, init_random=init_random)\n",
    "        yield emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 26s, sys: 313 ms, total: 7min 26s\n",
      "Wall time: 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fill custom_w2v\n",
    "for emb in emb_iter(df.question1.values):\n",
    "    continue\n",
    "for emb in emb_iter(df.question2.values):\n",
    "    continue\n",
    "for emb in emb_iter(test_df.question1.values):\n",
    "    continue\n",
    "for emb in emb_iter(test_df.question2.values):\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 30 04:39:33 MSK 2017\n",
      "CPU times: user 13min 58s, sys: 37.4 s, total: 14min 35s\n",
      "Wall time: 24min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!date\n",
    "\n",
    "# Train\n",
    "!mkdir -p bclz/train\n",
    "save_bcolz_array(emb_iter(train_df.question1.values), 'bclz/train/q1_%s' % bcolz_prefix)\n",
    "save_bcolz_array(emb_iter(train_df.question2.values), 'bclz/train/q2_%s' % bcolz_prefix)\n",
    "save_bcolz_array(iter(train_df.is_duplicate.values.astype(float)), 'bclz/train/y_%s' % bcolz_chunklen)\n",
    "\n",
    "# Validation\n",
    "!mkdir -p bclz/val\n",
    "save_bcolz_array(emb_iter(val_df.question1.values), 'bclz/val/q1_%s' % bcolz_prefix)\n",
    "save_bcolz_array(emb_iter(val_df.question2.values), 'bclz/val/q2_%s' % bcolz_prefix)\n",
    "save_bcolz_array(iter(val_df.is_duplicate.values.astype(float)), 'bclz/val/y_%s' % bcolz_chunklen)\n",
    "\n",
    "# Test \n",
    "!mkdir -p bclz/test\n",
    "save_bcolz_array(emb_iter(test_df.question1.values), 'bclz/test/q1_%s' % bcolz_prefix)\n",
    "save_bcolz_array(emb_iter(test_df.question2.values), 'bclz/test/q2_%s' % bcolz_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "def text_to_pos(text):\n",
    "    return [w.pos_ for w in nlp(text)]\n",
    "\n",
    "def seq_to_pos(sequence):\n",
    "    text = ' '.join(sequence)\n",
    "    return ' '.join([w.pos_ for w in nlp(text)])\n",
    "\n",
    "def pos_iter(data, tokenizer):\n",
    "    for qid, question in enumerate(data):\n",
    "        q_pos = seq_to_pos(text_to_wordlist(question))\n",
    "        seq = tokenizer.texts_to_sequences([q_pos])\n",
    "        seq = pad_sequences(seq, MAX_SEQUENCE_LENGTH, padding='post')\n",
    "        yield np.array(seq[0]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([seq_to_pos(text_to_wordlist(t))\n",
    "                        for t in train_df.question1.values[:10000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 30min 38s, sys: 20.5 s, total: 1h 30min 59s\n",
      "Wall time: 1h 43min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!date\n",
    "\n",
    "# Train\n",
    "save_bcolz_array(pos_iter(train_df.question1.values, tokenizer), 'bclz/train/pos1_%s' % bcolz_prefix)\n",
    "save_bcolz_array(pos_iter(train_df.question2.values, tokenizer), 'bclz/train/pos2_%s' % bcolz_prefix)\n",
    "\n",
    "# Validation\n",
    "save_bcolz_array(pos_iter(val_df.question1.values, tokenizer), 'bclz/val/pos1_%s' % bcolz_prefix)\n",
    "save_bcolz_array(pos_iter(val_df.question2.values, tokenizer), 'bclz/val/pos2_%s' % bcolz_prefix)\n",
    "\n",
    "# Test\n",
    "save_bcolz_array(pos_iter(test_df.question1.values, tokenizer), 'bclz/test/pos1_%s' % bcolz_prefix)\n",
    "save_bcolz_array(pos_iter(test_df.question2.values, tokenizer), 'bclz/test/pos2_%s' % bcolz_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "X1_train_pos = bcolz.open('bclz/train/pos1_%s' % bcolz_prefix)\n",
    "X2_train_pos = bcolz.open('bclz/train/pos2_%s' % bcolz_prefix)\n",
    "\n",
    "# Validation\n",
    "X1_val_pos = bcolz.open('bclz/val/pos1_%s' % bcolz_prefix)\n",
    "X2_val_pos = bcolz.open('bclz/val/pos2_%s' % bcolz_prefix)\n",
    "\n",
    "# Test\n",
    "X1_test_pos = bcolz.open('bclz/test/pos1_%s' % bcolz_prefix)\n",
    "X2_test_pos = bcolz.open('bclz/test/pos2_%s' % bcolz_prefix)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
